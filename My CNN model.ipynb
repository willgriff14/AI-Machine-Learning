{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will use a CNN since this baseline model gives higher accruacy than MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the data\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "(x_train, labels_train), (x_test, labels_test) = mnist.load_data()\n",
    "# convert to float\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255 # this is shorthand for x = x / 255\n",
    "x_test /= 255\n",
    "# make labels categories\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train = to_categorical(labels_train, 10)\n",
    "y_test = to_categorical(labels_test, 10)\n",
    "# data will be used as input to a convolutional layer\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "\n",
    "# define 3 layer CNN\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Dropout, Input\n",
    "inputs = Input(shape=x_train.shape[1:])\n",
    "x = Conv2D(filters=32, kernel_size=(5,5), activation='relu')(inputs)\n",
    "x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "x = Conv2D(filters=32, kernel_size=(3,3), activation='relu')(x)\n",
    "x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(rate=0.5)(x)\n",
    "outputs = Dense(10, activation='softmax')(x)\n",
    "net = Model(inputs=inputs, outputs=outputs)\n",
    "net.summary()\n",
    "\n",
    "# now we need to compile and train model\n",
    "net.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "history = net.fit(x_train, y_train, validation_data=(x_test, y_test),epochs=20,batch_size=256)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'], label='training loss')\n",
    "plt.plot(history.history['val_loss'], label='validation loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "net.save(\"network_for_mnist.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next I need to test my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs=net.predict(x_test)\n",
    "labels_predicted=np.argmax(outputs, axis=1)\n",
    "misclassified=sum(labels_predicted!=labels_test)\n",
    "print('Percentage misclassified = ',100*misclassified/labels_test.size)\n",
    "\n",
    "plt.figure(figsize=(8, 2))\n",
    "for i in range(0,8):\n",
    "    ax=plt.subplot(2,8,i+1)\n",
    "    plt.imshow(x_test[i,:].reshape(28,28), cmap=plt.get_cmap('gray_r'))\n",
    "    plt.title(labels_test[i])\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    \n",
    "for i in range(0,8):\n",
    "    output = net.predict(x_test[i,:].reshape(1, 28,28,1)) #if CNN\n",
    "    output=output[0,0:]\n",
    "    plt.subplot(2,8,8+i+1)\n",
    "    plt.bar(np.arange(10.),output)\n",
    "    plt.title(np.argmax(output))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
